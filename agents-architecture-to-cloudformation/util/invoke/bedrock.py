from botocore.exceptions import EventStreamError
from boto3.session import Session
from botocore.config import Config

import streamlit as st

from util.prompt_templates.explainPrompt import EXPLAIN_PROMPT
from util.prompt_templates.sys_explainPrompt import SYS_EXPLAIN_PROMPT

import time
import random
import uuid


def invoke_model(modelId, inference_params, messages, system_prompt, data_placeholder):
    """
    Invokes Amazon Bedrock Foundational model.

    Args:
        model(langchain_community.chat_models.bedrock.BedrockChat): Langchain bedrock chat instance.
        messages (list): List of Langchain SystemMessage and HumanMessage objects.
        data_placeholder (instanceof st.empty): Placeholder to stream the output.
    Returns:
        str: The response or output generated by the model.
    """
    bedrock = Session().client("bedrock-runtime", config=Config(read_timeout=600))
    result = str()
    response = bedrock.converse_stream(
        modelId=modelId,
        messages=messages,
        system=[{"text": system_prompt}],
        inferenceConfig={
            "maxTokens": 4000,
            "temperature": inference_params["temperature"],
            "topP": inference_params["top_p"],
        },
        additionalModelRequestFields={"top_k": inference_params["top_k"]},
    )

    stream = response.get("stream")
    if stream:
        for event in stream:

            if "contentBlockDelta" in event:
                result += event["contentBlockDelta"]["delta"]["text"]
                with data_placeholder.container():
                    st.text_area(
                        label="Step-by-step explain",
                        value=result,
                        height=500,
                        key=uuid.uuid4(),
                    )

    return result


def backoff_mechanism(
    func, modelId, inference_params, messages, system_prompt, data_placeholder
):
    """
    Implements a backoff mechanism to handle throttling exceptions.

    Args:
        func (function): The function to be called with backoff.
        model (langchain_community.chat_models.bedrock.BedrockChat): Langchain bedrock chat instance.
        messages (list): List of Langchain SystemMessage and HumanMessage objects.
        data_placeholder (instanceof st.empty): Placeholder to stream the output.

    Returns:
        str: The response or output generated by the model.
    """
    MAX_RETRIES = 5  # Maximum number of retries
    INITIAL_DELAY = 1  # Initial delay in seconds
    MAX_DELAY = 60  # Maximum delay in second

    delay = INITIAL_DELAY
    retries = 0

    while retries < MAX_RETRIES:
        try:
            return func(
                modelId=modelId,
                inference_params=inference_params,
                messages=messages,
                system_prompt=system_prompt,
                data_placeholder=data_placeholder,
            )
        except EventStreamError as e:
            print(f"Retry {retries + 1}/{MAX_RETRIES}: {e}")
            time.sleep(delay + random.uniform(0, 1))  # Add a random jitter
            delay = min(delay * 2, MAX_DELAY)
            retries += 1


class Bedrock:
    """
    Amazon Bedrock class to invoke Foundational Models. This class is used to generate AWS architecture explaination from architecture image.

    Usage:

    bedrock = Bedrock(inference_params=inference_params)

    # Generates the explaination of architecture diagram and streams the output to streamlit app.
    explain= bedrock.invoke_explain_model(image,image_type,explain_placeholder,)

    # Reset the session.
    bedrock.new_session()

    The class initializes session state on first run. It reuses the session for subsequent calls for continuity.
    """

    def __init__(self, inference_params):

        self._inference_params = inference_params

    def get_explain_messages(self, image, image_type):
        """
        Returns the messages for the explain model.
        Args:
            image (BytesIO): The image to explain.
            image_type (str): The type of the image.
        Returns:
            list: The list of messages.
        """
        messages = list()

        messages.append(
            {
                "role": "user",
                "content": [
                    {
                        "text": EXPLAIN_PROMPT,
                    },
                    {
                        "image": {
                            "format": image_type,
                            "source": {
                                "bytes": image.getvalue(),
                            },
                        },
                    },
                ],
            }
        )

        return SYS_EXPLAIN_PROMPT, messages

    def invoke_explain_model(self, image, image_type, data_placeholder):
        """
        Invokes the explain model.
        Args:
            image (BytesIO): The image to explain.
            image_type (str): The type of the image.
            data_placeholder (instanceof st.empty): Placeholder to stream the output.
        Returns:
            str: The response or output generated by the model.
        """

        system_prompt, messages = self.get_explain_messages(image, image_type)

        explain = backoff_mechanism(
            func=invoke_model,
            modelId="anthropic.claude-3-sonnet-20240229-v1:0",
            inference_params=self._inference_params,
            messages=messages,
            system_prompt=system_prompt,
            data_placeholder=data_placeholder,
        )
        return explain
