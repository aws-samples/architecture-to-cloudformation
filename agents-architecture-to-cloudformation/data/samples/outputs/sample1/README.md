# Output [Sample1.jpg](/agents-architecture-to-cloudformation/data/samples/sample1.jpg)

Let's generate AWS CloudFormation templates for the sample architecture diagram below.

![sample1.jpg](/agents-architecture-to-cloudformation/data/samples/sample1.jpg)

Uploading the architecture diagram to the web application generates a step-by-step explanation of the architecture diagram using Anthropic Claude 3 Sonnet vision capabilities. Before invoking Amazon Bedrock Agent, users have the ability to edit the step-by-step explanation.

![artifact-1.png](/agents-architecture-to-cloudformation/data/samples/outputs/sample1/artifact-1.png)

```
The main components and AWS services depicted in the diagram are:

1. Amazon EventBridge Schedule
2. AWS Lambda A
3. Amazon S3
4. AWS Lambda B
5. DynamoDB
6. DynamoDB Streams
7. Amazon SNS
8. Email

The flow of data and requests through the architecture is as follows:

1. The Amazon EventBridge Schedule triggers an event every day at 3 AM EDT.
2. This event invokes the AWS Lambda A function.
3. Lambda A reads data from the Amazon S3 bucket (data.txt).
4. Lambda A modifies the data (modify.txt) and creates an event in Amazon S3 bucket B.
5. The event created in Amazon S3 bucket B triggers AWS Lambda B.
6. Lambda B processes the object metadata and writes data to the DynamoDB table.
7. DynamoDB Streams captures the changes made to the DynamoDB table.
8. DynamoDB Streams triggers Amazon SNS, which sends an email notification.
```

Once you are satisfied with the step-by-step explanation, click on the InvokeAgent button. Invoking the agent results in the following output.

![artifact-2.mov](/agents-architecture-to-cloudformation/data/samples/outputs/sample1/artifact-2.mov)

The web application empowers users with the capability to modify the auto-generated AWS CloudFormation template through an integrated code editor. This feature encourages users to review and customize the template according to their specific requirements, reducing their dependence on the model's output. Once the necessary modifications have been made, users should ensure to click the `Submit` button to persist the updated template in the DynamoDB table.

![artifact-5.mov](/agents-architecture-to-cloudformation/data/samples/outputs/sample1/artifact-5.mov)

The web application provides visibility into the AWS templates that were fetched from the vector database. Furthermore, it offers the capability to download these retrieved templates locally.

![artifact-3.png](/agents-architecture-to-cloudformation/data/samples/outputs/sample1/artifact-3.png)

Once the first AWS CloudFormation template is generated, users have the capability to repeatedly update and tailor the template to align with their requirements by supplying update instructions via the chat panel.

![artifact-4.mov](/agents-architecture-to-cloudformation/data/samples/outputs/sample1/artifact-4.mov)


The AWS CloudFormation template that results from multiple iterations of this update process:

> [!IMPORTANT]  
> The AWS CloudFormation template generated by the web application serves as a reference or starting point for development purposes. It should not be directly utilized in production environments without proper testing and validation. Developers are responsible for thoroughly evaluating and modifying the CloudFormation template to ensure compliance with established security best practices and guidelines before deploying it to production systems.

```
AWSTemplateFormatVersion: '2010-09-09'
Description: >
  This template creates a serverless data processing pipeline using EventBridge, Lambda, S3, DynamoDB, and SNS.
  It schedules a daily event to trigger a Lambda function that reads data from S3, processes it, and stores it in another S3 bucket.
  Another Lambda function is triggered by the S3 event to process the data and store it in DynamoDB.
  DynamoDB Streams captures new items and triggers an SNS topic to send an email notification.
  This template is not production ready and should only be used for inspiration.

Parameters:
  S3BucketNameA:
    Type: String
    Description: Name of the S3 bucket for input data
    MinLength: 1
    MaxLength: 63
    AllowedPattern: ^[a-z0-9][a-z0-9-]*[a-z0-9]$

  S3BucketNameB:
    Type: String
    Description: Name of the S3 bucket for processed data
    MinLength: 1
    MaxLength: 63
    AllowedPattern: ^[a-z0-9][a-z0-9-]*[a-z0-9]$

  DynamoDBTableName:
    Type: String
    Description: Name of the DynamoDB table
    MinLength: 1
    MaxLength: 255

  EmailAddress:
    Type: String
    Description: Email address to receive notifications
    MinLength: 1

Resources:

  EventBridgeSchedule:
    Type: AWS::Events::Rule
    Properties:
      ScheduleExpression: 'cron(0 3 ? * * *)'
      State: ENABLED
      Targets:
        - Arn: !GetAtt LambdaFunctionA.Arn
          Id: LambdaFunctionA

  LambdaFunctionA:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: DataProcessingFunctionA
      Runtime: python3.12
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRoleA.Arn
      Code:
        ZipFile: |
          import boto3
          import os

          def lambda_handler(event, context):
              s3 = boto3.client('s3')
              s3_bucket_name_a = os.environ['S3_BUCKET_NAME_A']
              s3_bucket_name_b = os.environ['S3_BUCKET_NAME_B']

              s3.download_file(s3_bucket_name_a, 'data.txt', '/tmp/data.txt')

              with open('/tmp/data.txt', 'r') as f:
                  data = f.read()
              modified_data = data.upper()

              s3.upload_file('/tmp/modified.txt', s3_bucket_name_b, 'modified.txt')

              return {
                  'statusCode': 200,
                  'body': 'Data processing completed successfully'
              }
      Environment:
        Variables:
          S3_BUCKET_NAME_A: !Ref S3BucketNameA
          S3_BUCKET_NAME_B: !Ref S3BucketNameB

  LambdaRoleA:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                Resource: !Join ['', ['arn:aws:s3:::', !Ref S3BucketNameA, '/*']]
              - Effect: Allow
                Action:
                  - 's3:PutObject'
                Resource: !Join ['', ['arn:aws:s3:::', !Ref S3BucketNameB, '/*']]

  S3BucketA:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Join ['-', [!Ref S3BucketNameA, !Ref 'AWS::AccountId']]
      VersioningConfiguration:
        Status: Enabled

  S3BucketB:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Join ['-', [!Ref S3BucketNameB, !Ref 'AWS::AccountId']]
      VersioningConfiguration:
        Status: Enabled
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: 's3:ObjectCreated:*'
            Function: !GetAtt LambdaFunctionB.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .txt

  LambdaFunctionBPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !GetAtt LambdaFunctionB.Arn
      Principal: s3.amazonaws.com
      SourceArn: !Join ['', ['arn:aws:s3:::', !Join ['-', [!Ref S3BucketNameB, !Ref 'AWS::AccountId']]]]

  LambdaFunctionB:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: DataProcessingFunctionB
      Runtime: python3.12
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRoleB.Arn
      Code:
        ZipFile: |
          import boto3
          import json
          import os

          def lambda_handler(event, context):
              dynamodb = boto3.resource('dynamodb')
              table_name = os.environ['DYNAMODB_TABLE_NAME']
              table = dynamodb.Table(table_name)

              for record in event['Records']:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  metadata = record['s3']['object']

                  table.put_item(Item={
                      'ObjectKey': key,
                      'Metadata': metadata
                  })

              return {
                  'statusCode': 200,
                  'body': 'Data processing completed successfully'
              }
      Environment:
        Variables:
          DYNAMODB_TABLE_NAME: !Ref DynamoDBTableName

  LambdaRoleB:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'dynamodb:PutItem'
                Resource: !GetAtt DynamoDBTable.Arn

  DynamoDBTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Ref DynamoDBTableName
      BillingMode: PROVISIONED
      AttributeDefinitions:
        - AttributeName: ObjectKey
          AttributeType: S
      KeySchema:
        - AttributeName: ObjectKey
          KeyType: HASH
      ProvisionedThroughput:
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5
      StreamSpecification:
        StreamViewType: NEW_IMAGE

  DynamoDBStream:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      EventSourceArn: !GetAtt DynamoDBTable.StreamArn
      FunctionName: !GetAtt LambdaFunctionC.Arn
      StartingPosition: LATEST
      BatchSize: 1

  LambdaFunctionC:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: NotificationFunction
      Runtime: python3.12
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRoleC.Arn
      Code:
        ZipFile: |
          import boto3
          import os

          def lambda_handler(event, context):
              sns = boto3.client('sns')
              topic_arn = os.environ['SNS_TOPIC_ARN']

              for record in event['Records']:
                  if record['eventName'] == 'INSERT':
                      new_item = record['dynamodb']['NewImage']
                      item_id = new_item['ObjectKey']['S']
                      message = f"New item inserted: {item_id}"

                      sns.publish(
                          TopicArn=topic_arn,
                          Message=message,
                          Subject='Data Processing Notification'
                      )

              return {
                  'statusCode': 200,
                  'body': 'Notification sent successfully'
              }
      Environment:
        Variables:
          SNS_TOPIC_ARN: !Ref SNSTopic

  LambdaRoleC:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: SNSPublishAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'sns:Publish'
                Resource: !Ref SNSTopic
        - PolicyName: DynamoDBStreamAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'dynamodb:GetRecords'
                  - 'dynamodb:GetShardIterator'
                  - 'dynamodb:DescribeStream'
                  - 'dynamodb:ListStreams'
                Resource: !GetAtt DynamoDBTable.StreamArn
  SNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: DataProcessingTopic
      Subscription:
        - Endpoint: !Ref EmailAddress
          Protocol: email

Outputs:

  S3BucketNameA:
    Description: Name of the S3 bucket for input data
    Value: !Ref S3BucketA
    Export:
      Name: !Sub '${AWS::StackName}-S3BucketNameA'

  S3BucketNameB:
    Description: Name of the S3 bucket for processed data
    Value: !Ref S3BucketB
    Export:
      Name: !Sub '${AWS::StackName}-S3BucketNameB'

  DynamoDBTableName:
    Description: Name of the DynamoDB table
    Value: !Ref DynamoDBTable
    Export:
      Name: !Sub '${AWS::StackName}-DynamoDBTableName'

  SNSTopicArn:
    Description: ARN of the SNS topic for notifications
    Value: !Ref SNSTopic
    Export:
      Name: !Sub '${AWS::StackName}-SNSTopicArn'
```